{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OReUjfCcFJoC"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import load\n",
        "import transformers as ppb\n",
        "from collections import namedtuple\n",
        "from typing import List, Dict, Tuple\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "from joblib import dump, load"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAquwc9xtSkT"
      },
      "source": [
        "#import tensorflow as tf\n",
        "# Get the GPU device name.\n",
        "#device_name = tf.test.gpu_device_name()\n",
        "# The device name should look like the following:\n",
        "#if device_name == '/device:GPU:0':\n",
        "#    print('Found GPU at: {}'.format(device_name))\n",
        "#else:\n",
        "#    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "#if torch.cuda.is_available():      \n",
        "#    device = torch.device(\"cuda\")\n",
        "#    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "#    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "#else:\n",
        "#print('No GPU available, using the CPU instead.')\n",
        "#device = torch.device(\"cpu\")"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nziUfzhWFhvS",
        "outputId": "7e4ec012-e634-4538-f910-6fe0665d7bc4"
      },
      "source": [
        "#import bert model \n",
        "bert_model = \"distilbert-base-uncased\"\n",
        "tokenizer = ppb.DistilBertTokenizer.from_pretrained(bert_model)\n",
        "model = ppb.DistilBertModel.from_pretrained(bert_model)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GqToCkDFd81"
      },
      "source": [
        "df = pd.read_csv('final_df.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "pOFHrgy9cYaY",
        "outputId": "0c582f97-a1c7-4ee5-bcea-dc3921689fbd"
      },
      "source": [
        "#select fiction genres\n",
        "df = pd.read_csv('final_df.csv')\n",
        "df_fiction = df[['book','genres_Fiction']]\n",
        "\n",
        "pos_sample = df[df['genres_Fiction'] == True].sample(2500)\n",
        "neg_sample = df[df['genres_Fiction'] == False].sample(2500)\n",
        "goodreads_for_tokenizer = pd.concat([pos_sample, neg_sample])\n",
        "\n",
        "#drop unnecessary columns\n",
        "goodreads_for_tokenizer = goodreads_for_tokenizer[['book','genres_Fiction']].reset_index(drop = True)\n",
        "goodreads_for_tokenizer.head(2)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>book</th>\n",
              "      <th>genres_Fiction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Pearl/The Red Pony THE PEARLBased on an ol...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rachel's Holiday Meet Rachel Walsh. She has a ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                book  genres_Fiction\n",
              "0  The Pearl/The Red Pony THE PEARLBased on an ol...            True\n",
              "1  Rachel's Holiday Meet Rachel Walsh. She has a ...            True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6WEgjAuLT-M"
      },
      "source": [
        "#tokenize\n",
        "def tokenize_book(input_sentence):\n",
        "    input_ids = torch.tensor(tokenizer.encode(input_sentence, add_special_tokens=True)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    featurized_text = outputs[0][:,0,:].detach().numpy()\n",
        "    return featurized_text\n",
        "\n",
        "#to pad:\n",
        "def padder(tokenized):\n",
        "    max_len = 0\n",
        "    for i in tokenized.values:\n",
        "        if len(i) > max_len:\n",
        "            max_len = len(i)\n",
        "    return np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL54DZpYc1Ny",
        "outputId": "1d00db70-61d3-42a0-e435-cbe4d633243b"
      },
      "source": [
        "#tokenize and pad:\n",
        "tokenized = goodreads_for_tokenizer['book'].apply((lambda x: tokenizer.encode(x[:512], add_special_tokens=True)))\n",
        "padded = padder(tokenized)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 510)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlXT3DMhNsNn"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "tokenized = goodreads_for_tokenizer['book'].apply((lambda x: tokenizer.encode(x[:512], add_special_tokens=True)))\n",
        "padded = padder(tokenized)\n",
        "\n",
        "# Create the DataLoaders for our training\n",
        "train_dataloader = DataLoader(\n",
        "            padded,  # The training samples.\n",
        "            sampler = SequentialSampler(padded), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "feature = []\n",
        "\n",
        "with torch.no_grad(): \n",
        "  for d in train_dataloader:\n",
        "    input_ids = torch.tensor(np.array(d))\n",
        "    attention_mask = np.where(d != 0, 1, 0)\n",
        "    attention_mask = torch.tensor(attention_mask) \n",
        "\n",
        "    last_hidden_states = model(input_ids,attention_mask=attention_mask) \n",
        "    feature.extend(last_hidden_states[0][:,0,:].numpy())\n",
        "del d\n",
        "\n",
        "labels = goodreads_for_tokenizer['genres_Fiction']"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVVOVY-GOeN_",
        "outputId": "c2a31cbc-489d-4028-bfaa-a9c09f488cf4"
      },
      "source": [
        "#train logistic regression based on fiction genres\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(train_x, train_y)\n",
        "lr_clf = LogisticRegression(max_iter=5000)\n",
        "lr_clf.fit(train_features, train_labels)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBOGlcp7Ofxo",
        "outputId": "c976f2af-5663-43ad-9816-06ff90396a3d"
      },
      "source": [
        "lr_clf.score(test_features, test_labels)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaXwItN9bhcT",
        "outputId": "50fd1068-99b7-49ba-f327-2341514a5870"
      },
      "source": [
        "#f1 score model:\n",
        "f1_score(lr_clf.predict(test_features), test_labels)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8687350835322195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYFYgkGqbMxl"
      },
      "source": [
        "#check all genres\n",
        "genres = ['Literature','Fiction','Classics','Nonfiction','Historical','Novels','Fantasy','Childrens','Cultural','Mystery',\n",
        "         'History','Biography','Religion','European Literature','Thriller','Romance','Humor','Contemporary','Young Adult',\n",
        "         'Philosophy']"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "TlR3C1cWbbwP",
        "outputId": "f3570222-52c0-4ba0-c0f7-75efbcdce27d"
      },
      "source": [
        "#wrap up codes and apply on all genres\n",
        "batch_size = 32\n",
        "for genre in genres:\n",
        "    #sample 2500 datapoints and balance the dataset\n",
        "    pos_sample = df[df['genres_{}'.format(genre)] == True].sample(2500)\n",
        "    neg_sample = df[df['genres_{}'.format(genre)] == False].sample(2500)\n",
        "    goodreads_for_tokenizer = pd.concat([pos_sample, neg_sample])\n",
        "\n",
        "    #drop unnecessary columns\n",
        "    goodreads_for_tokenizer = goodreads_for_tokenizer[['book','genres_{}'.format(genre)]].reset_index(drop = True)\n",
        "\n",
        "    #tokenize and pad:\n",
        "    tokenized = goodreads_for_tokenizer['book'].apply((lambda x: tokenizer.encode(x[:512], add_special_tokens=True)))\n",
        "    padded = padder(tokenized)\n",
        "\n",
        "    # Create the DataLoaders for our training \n",
        "    train_dataloader = DataLoader(\n",
        "            padded,  # The training samples.\n",
        "            sampler = SequentialSampler(padded), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "    feature = []\n",
        "\n",
        "    with torch.no_grad(): \n",
        "      for d in train_dataloader:\n",
        "        input_ids = torch.tensor(np.array(d))\n",
        "        attention_mask = np.where(d != 0, 1, 0)\n",
        "        attention_mask = torch.tensor(attention_mask) \n",
        "\n",
        "        last_hidden_states = model(input_ids,attention_mask=attention_mask) \n",
        "        feature.extend(last_hidden_states[0][:,0,:].numpy())\n",
        "      del d\n",
        "\n",
        "    #genre-specific labels:\n",
        "    labels = goodreads_for_tokenizer['genres_{}'.format(genre)]\n",
        "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
        "    lr_clf = LogisticRegression(max_iter=5000)\n",
        "    lr_clf.fit(train_features, train_labels) #fit the model\n",
        "    #return a score:\n",
        "    print(genre + \" Accuracy Score= \" + str(lr_clf.score(test_features, test_labels)))\n",
        "    print(genre + \" F1 Score= \" + str(f1_score(lr_clf.predict(test_features), test_labels)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-07bd3ada2978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#genre-specific labels:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoodreads_for_tokenizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genres_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mlr_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlr_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2116\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2118\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [400, 5000]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmF2XwKbcz-2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}